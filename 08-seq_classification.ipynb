{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# An example of many-to-one (sequence classification)\n\nOriginal experiment from [Hochreiter & Schmidhuber (1997)](www.bioinf.jku.at/publications/older/2604.pdf).\n\nThe goal is to classify sequences.\nElements and targets are represented locally (input vectors with only one non-zero bit).\nThe sequence starts with an `B`, ends with a `E` (the “trigger symbol”), and otherwise consists of randomly chosen symbols from the set `{a, b, c, d}` except for two elements at positions `t1` and `t2` that are either `X` or `Y`.\nFor the `DifficultyLevel.HARD` case, the sequence length is randomly chosen between `100` and `110`, `t1` is randomly chosen between `10` and `20`, and `t2` is randomly chosen between `50` and `60`.\nThere are `4` sequence classes `Q`, `R`, `S`, and `U`, which depend on the temporal order of `X` and `Y`.\n\nThe rules are:\n\n```\nX, X -> Q,\nX, Y -> R,\nY, X -> S,\nY, Y -> U.\n```",
   "metadata": {
    "cell_id": "00000-cff4959a-359e-442d-a385-27952d4740c9",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Dataset Exploration",
   "metadata": {
    "cell_id": "00001-ab45b9bc-f108-4a0e-88b0-85cc3a3817a5",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00002-b5726fac-121f-4f45-9977-059630e826a7",
    "deepnote_cell_type": "code"
   },
   "source": "from res.sequential_tasks import TemporalOrderExp6aSequence as QRSU",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00003-a10029dc-d04a-4e69-89fc-b91e48334afb",
    "deepnote_cell_type": "code"
   },
   "source": "# Create a data generator\nexample_generator = QRSU.get_predefined_generator(\n    difficulty_level=QRSU.DifficultyLevel.EASY,\n    batch_size=32,\n)\n\nexample_batch = example_generator[1]\nprint(f'The return type is a {type(example_batch)} with length {len(example_batch)}.')\nprint(f'The first item in the tuple is the batch of sequences with shape {example_batch[0].shape}.')\nprint(f'The first element in the batch of sequences is:\\n {example_batch[0][0, :, :]}')\nprint(f'The second item in the tuple is the corresponding batch of class labels with shape {example_batch[1].shape}.')\nprint(f'The first element in the batch of class labels is:\\n {example_batch[1][0, :]}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00004-bde7ffc2-ff32-4c9a-b811-b563bdb662e4",
    "deepnote_cell_type": "code"
   },
   "source": "# Decoding the first sequence\nsequence_decoded = example_generator.decode_x(example_batch[0][0, :, :])\nprint(f'The sequence is: {sequence_decoded}')\n\n# Decoding the class label of the first sequence\nclass_label_decoded = example_generator.decode_y(example_batch[1][0])\nprint(f'The class label is: {class_label_decoded}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Defining the Model",
   "metadata": {
    "cell_id": "00005-f6d6eb12-12e1-4b56-990f-cd3fbba1096c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00006-de12f1a8-b04f-4c67-afb7-556b7b6f96d7",
    "deepnote_cell_type": "code"
   },
   "source": "import torch\nimport torch.nn as nn\n\n# Set the random seed for reproducible results\ntorch.manual_seed(1)\n\nclass SimpleRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        # This just calls the base class constructor\n        super().__init__()\n        # Neural network layers assigned as attributes of a Module subclass\n        # have their parameters registered for training automatically.\n        self.rnn = torch.nn.RNN(input_size, hidden_size, nonlinearity='relu', batch_first=True)\n        self.linear = torch.nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        # The RNN also returns its hidden state but we don't use it.\n        # While the RNN can also take a hidden state as input, the RNN\n        # gets passed a hidden state initialized with zeros by default.\n        h = self.rnn(x)[0]\n        x = self.linear(h)\n        return x\n\nclass SimpleLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.lstm = torch.nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.linear = torch.nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        h = self.lstm(x)[0]\n        x = self.linear(h)\n        return x\n    \n    def get_states_across_time(self, x):\n        h_c = None\n        h_list, c_list = list(), list()\n        with torch.no_grad():\n            for t in range(x.size(1)):\n                h_c = self.lstm(x[:, [t], :], h_c)[1]\n                h_list.append(h_c[0])\n                c_list.append(h_c[1])\n            h = torch.cat(h_list)\n            c = torch.cat(c_list)\n        return h, c",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Defining the Training Loop",
   "metadata": {
    "cell_id": "00007-28f7b62b-5a69-46a6-a6f0-b5df3181b523",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00008-20e35132-7213-4b2a-b98d-625ee2d7cfbb",
    "deepnote_cell_type": "code"
   },
   "source": "def train(model, train_data_gen, criterion, optimizer, device):\n    # Set the model to training mode. This will turn on layers that would\n    # otherwise behave differently during evaluation, such as dropout.\n    model.train()\n\n    # Store the number of sequences that were classified correctly\n    num_correct = 0\n\n    # Iterate over every batch of sequences. Note that the length of a data generator\n    # is defined as the number of batches required to produce a total of roughly 1000\n    # sequences given a batch size.\n    for batch_idx in range(len(train_data_gen)):\n\n        # Request a batch of sequences and class labels, convert them into tensors\n        # of the correct type, and then send them to the appropriate device.\n        data, target = train_data_gen[batch_idx]\n        data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).long().to(device)\n\n        # Perform the forward pass of the model\n        output = model(data)  # Step ①\n\n        # Pick only the output corresponding to last sequence element (input is pre padded)\n        output = output[:, -1, :]\n\n        # Compute the value of the loss for this batch. For loss functions like CrossEntropyLoss,\n        # the second argument is actually expected to be a tensor of class indices rather than\n        # one-hot encoded class labels. One approach is to take advantage of the one-hot encoding\n        # of the target and call argmax along its second dimension to create a tensor of shape\n        # (batch_size) containing the index of the class label that was hot for each sequence.\n        target = target.argmax(dim=1)\n\n        loss = criterion(output, target)  # Step ②\n\n        # Clear the gradient buffers of the optimized parameters.\n        # Otherwise, gradients from the previous batch would be accumulated.\n        optimizer.zero_grad()  # Step ③\n\n        loss.backward()  # Step ④\n\n        optimizer.step()  # Step ⑤\n\n        y_pred = output.argmax(dim=1)\n        num_correct += (y_pred == target).sum().item()\n\n    return num_correct, loss.item()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Defining the Testing Loop",
   "metadata": {
    "cell_id": "00009-f2d76288-181b-48ab-a3b8-59e4c91a9c42",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00010-056ace9c-ca18-4578-9378-c42e3a4143f9",
    "deepnote_cell_type": "code"
   },
   "source": "def test(model, test_data_gen, criterion, device):\n    # Set the model to evaluation mode. This will turn off layers that would\n    # otherwise behave differently during training, such as dropout.\n    model.eval()\n\n    # Store the number of sequences that were classified correctly\n    num_correct = 0\n\n    # A context manager is used to disable gradient calculations during inference\n    # to reduce memory usage, as we typically don't need the gradients at this point.\n    with torch.no_grad():\n        for batch_idx in range(len(test_data_gen)):\n            data, target = test_data_gen[batch_idx]\n            data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).long().to(device)\n\n            output = model(data)\n            # Pick only the output corresponding to last sequence element (input is pre padded)\n            output = output[:, -1, :]\n\n            target = target.argmax(dim=1)\n            loss = criterion(output, target)\n\n            y_pred = output.argmax(dim=1)\n            num_correct += (y_pred == target).sum().item()\n\n    return num_correct, loss.item()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Putting it All Together",
   "metadata": {
    "cell_id": "00011-a574e2c2-d87a-492e-bc70-47d139413886",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00012-70526dae-aff2-4322-9c5b-59447f40cc1d",
    "deepnote_cell_type": "code"
   },
   "source": "import matplotlib.pyplot as plt\nfrom res.plot_lib import set_default, plot_state, print_colourbar",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00013-139701ee-6350-4ecf-95de-372dcdcaa064",
    "deepnote_cell_type": "code"
   },
   "source": "set_default()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00014-1748f6cf-7176-4977-9be7-db541874e323",
    "deepnote_cell_type": "code"
   },
   "source": "def train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs, verbose=True):\n    # Automatically determine the device that PyTorch should use for computation\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    # Move model to the device which will be used for train and test\n    model.to(device)\n\n    # Track the value of the loss function and model accuracy across epochs\n    history_train = {'loss': [], 'acc': []}\n    history_test = {'loss': [], 'acc': []}\n\n    for epoch in range(max_epochs):\n        # Run the training loop and calculate the accuracy.\n        # Remember that the length of a data generator is the number of batches,\n        # so we multiply it by the batch size to recover the total number of sequences.\n        num_correct, loss = train(model, train_data_gen, criterion, optimizer, device)\n        accuracy = float(num_correct) / (len(train_data_gen) * train_data_gen.batch_size) * 100\n        history_train['loss'].append(loss)\n        history_train['acc'].append(accuracy)\n\n        # Do the same for the testing loop\n        num_correct, loss = test(model, test_data_gen, criterion, device)\n        accuracy = float(num_correct) / (len(test_data_gen) * test_data_gen.batch_size) * 100\n        history_test['loss'].append(loss)\n        history_test['acc'].append(accuracy)\n\n        if verbose or epoch + 1 == max_epochs:\n            print(f'[Epoch {epoch + 1}/{max_epochs}]'\n                  f\" loss: {history_train['loss'][-1]:.4f}, acc: {history_train['acc'][-1]:2.2f}%\"\n                  f\" - test_loss: {history_test['loss'][-1]:.4f}, test_acc: {history_test['acc'][-1]:2.2f}%\")\n\n    # Generate diagnostic plots for the loss and accuracy\n    fig, axes = plt.subplots(ncols=2, figsize=(9, 4.5))\n    for ax, metric in zip(axes, ['loss', 'acc']):\n        ax.plot(history_train[metric])\n        ax.plot(history_test[metric])\n        ax.set_xlabel('epoch', fontsize=12)\n        ax.set_ylabel(metric, fontsize=12)\n        ax.legend(['Train', 'Test'], loc='best')\n    plt.show()\n\n    return model",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Simple RNN: 10 Epochs",
   "metadata": {
    "cell_id": "00015-8059c59f-2e98-4c0e-9fb0-1b917f62c9a5",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00016-07c6d50d-e8e8-46cb-a315-53a944f29216",
    "deepnote_cell_type": "code"
   },
   "source": "# Setup the training and test data generators\ndifficulty     = QRSU.DifficultyLevel.EASY\nbatch_size     = 32\ntrain_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\ntest_data_gen  = QRSU.get_predefined_generator(difficulty, batch_size)\n\n# Setup the RNN and training settings\ninput_size  = train_data_gen.n_symbols\nhidden_size = 4\noutput_size = train_data_gen.n_classes\nmodel       = SimpleRNN(input_size, hidden_size, output_size)\ncriterion   = torch.nn.CrossEntropyLoss()\noptimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\nmax_epochs  = 10\n\n# Train the model\nmodel = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00017-cb35c9b0-3ce7-4000-8816-7eb9689eec2a",
    "deepnote_cell_type": "code"
   },
   "source": "for parameter_group in list(model.parameters()):\n    print(parameter_group.size())",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Simple LSTM: 10 Epochs",
   "metadata": {
    "cell_id": "00018-ff9ba94f-ed58-46e4-9cde-559c3d337807",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00019-afebbcaf-10d4-455e-bb31-660018e4a86b",
    "deepnote_cell_type": "code"
   },
   "source": "# Setup the training and test data generators\ndifficulty     = QRSU.DifficultyLevel.EASY\nbatch_size     = 32\ntrain_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\ntest_data_gen  = QRSU.get_predefined_generator(difficulty, batch_size)\n\n# Setup the RNN and training settings\ninput_size  = train_data_gen.n_symbols\nhidden_size = 4\noutput_size = train_data_gen.n_classes\nmodel       = SimpleLSTM(input_size, hidden_size, output_size)\ncriterion   = torch.nn.CrossEntropyLoss()\noptimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\nmax_epochs  = 10\n\n# Train the model\nmodel = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00020-d9022971-ad3e-4a00-8d53-8067b2f15c0a",
    "deepnote_cell_type": "code"
   },
   "source": "for parameter_group in list(model.parameters()):\n    print(parameter_group.size())",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. RNN: Increasing Epoch to 100",
   "metadata": {
    "cell_id": "00021-b60cfc21-774d-4f7e-9e6e-3381eb004e26",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00022-b4722d3c-6a52-41b1-acfc-077f62062377",
    "deepnote_cell_type": "code"
   },
   "source": "# Setup the training and test data generators\ndifficulty     = QRSU.DifficultyLevel.EASY\nbatch_size     = 32\ntrain_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\ntest_data_gen  = QRSU.get_predefined_generator(difficulty, batch_size)\n\n# Setup the RNN and training settings\ninput_size  = train_data_gen.n_symbols\nhidden_size = 4\noutput_size = train_data_gen.n_classes\nmodel       = SimpleRNN(input_size, hidden_size, output_size)\ncriterion   = torch.nn.CrossEntropyLoss()\noptimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\nmax_epochs  = 100\n\n# Train the model\nmodel = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs, verbose=False)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## LSTM: Increasing Epoch to 100",
   "metadata": {
    "cell_id": "00023-bf61e424-5441-4829-b14a-bc696877965f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00024-041fc259-4407-43f1-b36a-aca94e3e4b76",
    "deepnote_cell_type": "code"
   },
   "source": "# Setup the training and test data generators\ndifficulty     = QRSU.DifficultyLevel.EASY\nbatch_size     = 32\ntrain_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\ntest_data_gen  = QRSU.get_predefined_generator(difficulty, batch_size)\n\n# Setup the RNN and training settings\ninput_size  = train_data_gen.n_symbols\nhidden_size = 4\noutput_size = train_data_gen.n_classes\nmodel       = SimpleLSTM(input_size, hidden_size, output_size)\ncriterion   = torch.nn.CrossEntropyLoss()\noptimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\nmax_epochs  = 100\n\n# Train the model\nmodel = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs, verbose=False)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Model Evaluation",
   "metadata": {
    "cell_id": "00025-0ad30df7-018e-4e89-8c20-63b30f621032",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00026-d34dddcd-5fcf-4d39-8b20-f6292abf75e8",
    "deepnote_cell_type": "code"
   },
   "source": "import collections\nimport random\n\ndef evaluate_model(model, difficulty, seed=9001, verbose=False):\n    # Define a dictionary that maps class indices to labels\n    class_idx_to_label = {0: 'Q', 1: 'R', 2: 'S', 3: 'U'}\n\n    # Create a new data generator\n    data_generator = QRSU.get_predefined_generator(difficulty, seed=seed)\n\n    # Track the number of times a class appears\n    count_classes = collections.Counter()\n\n    # Keep correctly classified and misclassified sequences, and their\n    # true and predicted class labels, for diagnostic information.\n    correct = []\n    incorrect = []\n\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    model.eval()\n\n    with torch.no_grad():\n        for batch_idx in range(len(data_generator)):\n            data, target = test_data_gen[batch_idx]\n            data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).long().to(device)\n\n            data_decoded = data_generator.decode_x_batch(data.cpu().numpy())\n            target_decoded = data_generator.decode_y_batch(target.cpu().numpy())\n\n            output = model(data)\n            sequence_end = torch.tensor([len(sequence) for sequence in data_decoded]) - 1\n            output = output[torch.arange(data.shape[0]).long(), sequence_end, :]\n\n            target = target.argmax(dim=1)\n            y_pred = output.argmax(dim=1)\n            y_pred_decoded = [class_idx_to_label[y.item()] for y in y_pred]\n\n            count_classes.update(target_decoded)\n            for i, (truth, prediction) in enumerate(zip(target_decoded, y_pred_decoded)):\n                if truth == prediction:\n                    correct.append((data_decoded[i], truth, prediction))\n                else:\n                    incorrect.append((data_decoded[i], truth, prediction))\n\n    num_sequences = sum(count_classes.values())\n    accuracy = float(len(correct)) / num_sequences * 100\n    print(f'The accuracy of the model is measured to be {accuracy:.2f}%.\\n')\n\n    # Report the accuracy by class\n    for label in sorted(count_classes):\n        num_correct = sum(1 for _, truth, _ in correct if truth == label)\n        print(f'{label}: {num_correct} / {count_classes[label]} correct')\n\n    # Report some random sequences for examination\n    print('\\nHere are some example sequences:')\n    for i in range(10):\n        sequence, truth, prediction = correct[random.randrange(0, 10)]\n        print(f'{sequence} -> {truth} was labelled {prediction}')\n\n    # Report misclassified sequences for investigation\n    if incorrect and verbose:\n        print('\\nThe following sequences were misclassified:')\n        for sequence, truth, prediction in incorrect:\n            print(f'{sequence} -> {truth} was labelled {prediction}')\n    else:\n        print('\\nThere were no misclassified sequences.')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00027-de4981e7-0f95-41b0-b32e-200fda360565",
    "deepnote_cell_type": "code"
   },
   "source": "evaluate_model(model, difficulty)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Visualize LSTM\nSetting difficulty to `MODERATE` and `hidden_size` to 12.",
   "metadata": {
    "cell_id": "00028-4a8d1d10-bdf2-4abe-9a65-b52bbecc4616",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "cell_id": "00029-a6568015-01f2-48da-b44a-d5fd61740fb8",
    "deepnote_cell_type": "code"
   },
   "source": "# For reproducibility\ntorch.manual_seed(1)\n\n# Setup the training and test data generators\ndifficulty     = QRSU.DifficultyLevel.MODERATE\nbatch_size     = 32\ntrain_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\ntest_data_gen  = QRSU.get_predefined_generator(difficulty, batch_size)\n\n# Setup the RNN and training settings\ninput_size  = train_data_gen.n_symbols\nhidden_size = 10\noutput_size = train_data_gen.n_classes\nmodel       = SimpleLSTM(input_size, hidden_size, output_size)\ncriterion   = torch.nn.CrossEntropyLoss()\noptimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\nmax_epochs  = 100\n\n# Train the model\nmodel = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs, verbose=False)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00030-4464089b-95a0-464f-bb50-b0624941b640",
    "deepnote_cell_type": "code"
   },
   "source": "# Get hidden (H) and cell (C) batch state given a batch input (X)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel.eval()\nwith torch.no_grad():\n    data = test_data_gen[0][0]\n    X = torch.from_numpy(data).float().to(device)\n    H_t, C_t = model.get_states_across_time(X)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00031-a4e930e4-0e5e-4777-b15a-78730b71429c",
    "deepnote_cell_type": "code"
   },
   "source": "print(\"Color range is as follows:\")\nprint_colourbar()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00032-517c51d2-61bf-49f2-b7e2-fe0af8da2448",
    "deepnote_cell_type": "code"
   },
   "source": "plot_state(X.cpu(), C_t, b=9, decoder=test_data_gen.decode_x)  # 3, 6, 9",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00033-a03df12e-006b-4e41-9161-a8a7b0c9eade",
    "deepnote_cell_type": "code"
   },
   "source": "plot_state(X.cpu(), H_t, b=9, decoder=test_data_gen.decode_x)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=310d7331-290c-4084-b4b3-9339860041ff' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "deepnote_notebook_id": "ec36f6ea-7da0-4573-a07c-2521285f64b6",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}